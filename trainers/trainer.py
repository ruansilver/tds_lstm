"""
æ¨¡å‹è®­ç»ƒå™¨
"""

import os
import time
import logging
from typing import Dict, Any, Optional, Tuple
import numpy as np

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader

from config import Config
from utils.metrics import calculate_metrics
from utils.checkpoint import CheckpointManager
from utils.early_stopping import EarlyStopping

logger = logging.getLogger(__name__)

# TensorBoardå¯¼å…¥å¤„ç†
try:
    from torch.utils.tensorboard import SummaryWriter
    TENSORBOARD_AVAILABLE = True
except ImportError:
    SummaryWriter = None
    TENSORBOARD_AVAILABLE = False
    logger.warning("TensorBoardä¸å¯ç”¨ï¼Œå°†è·³è¿‡å¯è§†åŒ–æ—¥å¿—")


class EMG2PoseTrainer:
    """EMGåˆ°å§¿æ€é¢„æµ‹çš„è®­ç»ƒå™¨"""
    
    def __init__(
        self,
        model: nn.Module,
        config: Config,
        train_loader: DataLoader,
        val_loader: DataLoader,
        test_loader: Optional[DataLoader] = None
    ):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        Args:
            model: è¦è®­ç»ƒçš„æ¨¡å‹
            config: é…ç½®å¯¹è±¡
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
            test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨ï¼ˆå¯é€‰ï¼‰
        """
        self.model = model
        self.config = config
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.test_loader = test_loader
        # è®¾å¤‡é…ç½®
        self.device = torch.device(config.training.device)
        self.model.to(self.device)
        
        # æŸå¤±å‡½æ•°
        self.criterion = self._create_criterion()
        
        # ä¼˜åŒ–å™¨
        self.optimizer = self._create_optimizer()
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = self._create_scheduler()
        self.warmup_scheduler = self._create_warmup_scheduler() if config.training.warmup_epochs > 0 else None
        
        # æ—©åœæœºåˆ¶
        if config.training.early_stopping:
            # ç¡®å®šç›‘æ§æ¨¡å¼ï¼ˆminè¿˜æ˜¯maxï¼‰
            monitor_metric = getattr(config.training, 'monitor_metric', 'val_loss')
            # å¯¹äºæŸå¤±å’Œè¯¯å·®ç±»æŒ‡æ ‡ä½¿ç”¨minï¼Œå¯¹äºç²¾åº¦ç±»æŒ‡æ ‡ä½¿ç”¨max
            mode = 'max' if 'acc' in monitor_metric.lower() or 'precision' in monitor_metric.lower() else 'min'
            
            self.early_stopping = EarlyStopping(
                patience=config.training.patience,
                min_delta=config.training.min_delta,
                mode=mode,
                restore_best_weights=getattr(config.training, 'restore_best_weights', True),
                verbose=True
            )
            self.monitor_metric = monitor_metric
        else:
            self.early_stopping = None
            self.monitor_metric = None
        
        # æ£€æŸ¥ç‚¹ç®¡ç†å™¨
        self.checkpoint_manager = CheckpointManager(
            checkpoint_dir=config.logging.checkpoint_dir,
            save_best_only=config.logging.save_best_only,
            monitor=config.logging.monitor,
            mode=config.logging.mode
        )
        
        # TensorBoardå†™å…¥å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if TENSORBOARD_AVAILABLE and SummaryWriter is not None:
            try:
                self.writer = SummaryWriter(config.logging.log_dir)
                logger.info(f"âœ… TensorBoardåˆå§‹åŒ–æˆåŠŸï¼Œæ—¥å¿—ç›®å½•: {config.logging.log_dir}")
            except Exception as e:
                logger.warning(f"âš ï¸ TensorBoardåˆå§‹åŒ–å¤±è´¥: {e}")
                self.writer = None
        else:
            self.writer = None
        
        # è®­ç»ƒçŠ¶æ€
        self.current_epoch = 0    # å½“å‰è®­ç»ƒçš„epoch
        self.global_step = 0     # å…¨å±€è®­ç»ƒæ­¥æ•°
        self.best_val_loss = float('inf')    # æœ€ä½³éªŒè¯æŸå¤±
        self.train_history = {'loss': [], 'metrics': []}    # è®­ç»ƒå†å²è®°å½•
        self.val_history = {'loss': [], 'metrics': []}   # éªŒè¯å†å²è®°å½•
        
    def _create_criterion(self):
        """åˆ›å»ºæŸå¤±å‡½æ•°"""
        loss_type = getattr(self.config.training, 'loss_type', 'mse')
        
        if loss_type.lower() == 'mse':
            return nn.MSELoss()
        elif loss_type.lower() == 'mae':
            return nn.L1Loss()
        elif loss_type.lower() == 'mse+mae':
            # ç»„åˆæŸå¤±
            mae_weight = getattr(self.config.training, 'mae_weight', 0.5)
            mse_criterion = nn.MSELoss()
            mae_criterion = nn.L1Loss()
            
            def combined_loss(pred, target):
                mse = mse_criterion(pred, target)
                mae = mae_criterion(pred, target)
                return mse + mae_weight * mae
            
            return combined_loss
        else:
            logger.warning(f"æœªçŸ¥çš„æŸå¤±ç±»å‹: {loss_type}ï¼Œä½¿ç”¨é»˜è®¤MSE")
            return nn.MSELoss()
    
    def _create_optimizer(self) -> optim.Optimizer:
        """åˆ›å»ºä¼˜åŒ–å™¨"""
        if self.config.training.optimizer.lower() == 'adam':
            return optim.Adam(
                self.model.parameters(),
                lr=self.config.training.learning_rate,
                weight_decay=self.config.training.weight_decay
            )
        elif self.config.training.optimizer.lower() == 'adamw':
            return optim.AdamW(
                self.model.parameters(),
                lr=self.config.training.learning_rate,
                weight_decay=self.config.training.weight_decay
            )
        elif self.config.training.optimizer.lower() == 'sgd':
            return optim.SGD(
                self.model.parameters(),     
                lr=self.config.training.learning_rate,
                momentum=0.9,
                weight_decay=self.config.training.weight_decay
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ä¼˜åŒ–å™¨: {self.config.training.optimizer}")
    
    def _create_scheduler(self) -> Optional[optim.lr_scheduler._LRScheduler]:
        """åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨"""
        scheduler_type = self.config.training.scheduler.lower()
        
        if scheduler_type == 'step':
            return optim.lr_scheduler.StepLR(
                self.optimizer,
                step_size=self.config.training.step_size,
                gamma=self.config.training.gamma
            )
        elif scheduler_type == 'cosine':
            return optim.lr_scheduler.CosineAnnealingLR(
                self.optimizer,
                T_max=self.config.training.num_epochs
            )
        elif scheduler_type == 'plateau':  
            return optim.lr_scheduler.ReduceLROnPlateau(
                self.optimizer,
                mode='min',
                factor=0.5,
                patience=5,
                verbose=True
            )
        elif scheduler_type == 'none':
            return None
        else:
            return None
    
    def _create_warmup_scheduler(self):
        """åˆ›å»ºwarmupå­¦ä¹ ç‡è°ƒåº¦å™¨"""
        warmup_epochs = getattr(self.config.training, 'warmup_epochs', 0)
        if warmup_epochs <= 0:
            return None
        
        # ä½¿ç”¨çº¿æ€§warmup
        def warmup_lambda(epoch):
            if epoch < warmup_epochs:
                return (epoch + 1) / warmup_epochs
            return 1.0
        
        return optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda=warmup_lambda)
    
    def train_epoch(self) -> Dict[str, float]:   
        """è®­ç»ƒä¸€ä¸ªepochï¼ˆå†…å­˜ä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
        self.model.train()  # è®¾ç½®æ¨¡å‹ä¸ºè®­ç»ƒæ¨¡å¼
        total_loss = 0.0   # ç´¯è®¡æŸå¤±
        
        # å†…å­˜ä¼˜åŒ–ï¼šåœ¨çº¿è®¡ç®—metricsï¼Œä¸å­˜å‚¨æ‰€æœ‰é¢„æµ‹
        compute_metrics_online = getattr(self.config.training, 'compute_metrics_online', True)
        
        if compute_metrics_online:
            # åœ¨çº¿ç»Ÿè®¡é‡
            num_samples = 0
            sum_squared_error = 0.0
            sum_absolute_error = 0.0
        else:
            # ä¼ ç»Ÿæ–¹å¼ï¼šæ”¶é›†æ ·æœ¬ï¼ˆä»…ç”¨äºå°æ•°æ®é›†ï¼‰
            all_predictions = []
            all_targets = []
        
        # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
        accumulation_steps = getattr(self.config.training, 'gradient_accumulation_steps', 1)
        
        for batch_idx, (emg_data, angle_data) in enumerate(self.train_loader):
            # æ•°æ®ç§»åˆ°è®¾å¤‡
            emg_data = emg_data.to(self.device)
            angle_data = angle_data.to(self.device)
            
            # å‰å‘ä¼ æ’­
            predictions = self.model(emg_data)
            loss = self.criterion(predictions, angle_data)
            
            # æ¢¯åº¦ç´¯ç§¯ï¼šæŸå¤±éœ€è¦é™¤ä»¥ç´¯ç§¯æ­¥æ•°
            loss = loss / accumulation_steps
            
            # åå‘ä¼ æ’­
            loss.backward()
            
            # æ¯accumulation_stepsæ­¥æˆ–æœ€åä¸€æ­¥æ‰æ›´æ–°å‚æ•°
            if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == len(self.train_loader):
                # æ¢¯åº¦è£å‰ª
                grad_clip_norm = getattr(self.config.training, 'gradient_clip_norm', 1.0)
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=grad_clip_norm)
                
                # ä¼˜åŒ–å™¨æ­¥éª¤
                self.optimizer.step()
                self.optimizer.zero_grad()
            
            # ç»Ÿè®¡æŸå¤±ï¼ˆä½¿ç”¨åŸå§‹lossï¼Œä¸æ˜¯é™¤ä»¥accumulation_stepsåçš„ï¼‰
            total_loss += loss.item() * accumulation_steps
            
            # åœ¨çº¿è®¡ç®—metricsï¼ˆå†…å­˜å‹å¥½ï¼‰
            if compute_metrics_online:
                with torch.no_grad():
                    batch_size = predictions.shape[0]
                    num_samples += batch_size * predictions.shape[1]
                    
                    # è®¡ç®—è¯¯å·®ç»Ÿè®¡é‡
                    squared_error = ((predictions - angle_data) ** 2).sum().item()
                    absolute_error = torch.abs(predictions - angle_data).sum().item()
                    
                    sum_squared_error += squared_error
                    sum_absolute_error += absolute_error
            else:
                # ä¼ ç»Ÿæ–¹å¼ï¼šæ”¶é›†æ ·æœ¬
                all_predictions.append(predictions.detach().cpu().numpy())
                all_targets.append(angle_data.detach().cpu().numpy())
            
            # å†…å­˜æ¸…ç†ï¼šåŠæ—¶é‡Šæ”¾ä¸éœ€è¦çš„tensor
            del emg_data, angle_data, predictions, loss
            
            # æ›´æ¿€è¿›çš„å†…å­˜æ¸…ç†
            if batch_idx % 50 == 0:
                torch.cuda.empty_cache() if torch.cuda.is_available() else None
            
            # æ—¥å¿—è®°å½•
            if batch_idx % self.config.logging.log_every == 0:
                current_loss = total_loss / (batch_idx + 1)
                if self.writer:
                    self.writer.add_scalar('Train/BatchLoss', current_loss, self.global_step)
                    self.writer.add_scalar('Train/LearningRate', 
                                         self.optimizer.param_groups[0]['lr'], self.global_step)
                
                logger.info(f'Epoch: {self.current_epoch}, Batch: {batch_idx}/{len(self.train_loader)}, '
                           f'Loss: {current_loss:.6f}')
            
            self.global_step += 1
        
        # è®¡ç®—epochå¹³å‡æŸå¤±å’ŒæŒ‡æ ‡
        avg_loss = total_loss / len(self.train_loader)
        
        # è®¡ç®—metrics
        if compute_metrics_online:
            # ä»åœ¨çº¿ç»Ÿè®¡é‡è®¡ç®—metrics
            metrics = {
                'mse': sum_squared_error / num_samples,
                'mae': sum_absolute_error / num_samples,
                'rmse': np.sqrt(sum_squared_error / num_samples)
            }
        else:
            # ä¼ ç»Ÿæ–¹å¼ï¼šä»æ”¶é›†çš„æ ·æœ¬è®¡ç®—
            # å†…å­˜ä¼˜åŒ–ï¼šé™åˆ¶æ ·æœ¬æ•°
            if len(all_predictions) > 1000:
                all_predictions = all_predictions[:1000]
                all_targets = all_targets[:1000]
            
            all_predictions = np.concatenate(all_predictions, axis=0)
            all_targets = np.concatenate(all_targets, axis=0)
            metrics = calculate_metrics(all_targets, all_predictions)
        
        return {'loss': avg_loss, **metrics}
    
    def validate_epoch(self) -> Dict[str, float]:
        """éªŒè¯ä¸€ä¸ªepochï¼ˆå†…å­˜ä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
        self.model.eval()
        total_loss = 0.0
        
        # å†…å­˜ä¼˜åŒ–ï¼šåœ¨çº¿è®¡ç®—metrics
        compute_metrics_online = getattr(self.config.training, 'compute_metrics_online', True)
        
        if compute_metrics_online:
            num_samples = 0
            sum_squared_error = 0.0
            sum_absolute_error = 0.0
        else:
            all_predictions = []
            all_targets = []
        
        with torch.no_grad():
            for batch_idx, (emg_data, angle_data) in enumerate(self.val_loader):
                # æ•°æ®ç§»åˆ°è®¾å¤‡
                emg_data = emg_data.to(self.device)
                angle_data = angle_data.to(self.device)
                
                # å‰å‘ä¼ æ’­
                predictions = self.model(emg_data)
                loss = self.criterion(predictions, angle_data)
                
                # ç»Ÿè®¡æŸå¤±
                total_loss += loss.item()
                
                # åœ¨çº¿è®¡ç®—metrics
                if compute_metrics_online:
                    batch_size = predictions.shape[0]
                    num_samples += batch_size * predictions.shape[1]
                    
                    squared_error = ((predictions - angle_data) ** 2).sum().item()
                    absolute_error = torch.abs(predictions - angle_data).sum().item()
                    
                    sum_squared_error += squared_error
                    sum_absolute_error += absolute_error
                else:
                    all_predictions.append(predictions.cpu().numpy())
                    all_targets.append(angle_data.cpu().numpy())
                
                # å†…å­˜æ¸…ç†
                del emg_data, angle_data, predictions, loss
                
                if batch_idx % 50 == 0:
                    torch.cuda.empty_cache() if torch.cuda.is_available() else None
        
        # è®¡ç®—å¹³å‡æŸå¤±å’ŒæŒ‡æ ‡
        avg_loss = total_loss / len(self.val_loader)
        
        if compute_metrics_online:
            metrics = {
                'mse': sum_squared_error / num_samples,
                'mae': sum_absolute_error / num_samples,
                'rmse': np.sqrt(sum_squared_error / num_samples)
            }
        else:
            # å†…å­˜ä¼˜åŒ–ï¼šé™åˆ¶æ ·æœ¬æ•°
            if len(all_predictions) > 500:
                all_predictions = all_predictions[:500]
                all_targets = all_targets[:500]
            
            all_predictions = np.concatenate(all_predictions, axis=0)
            all_targets = np.concatenate(all_targets, axis=0)
            metrics = calculate_metrics(all_targets, all_predictions)
        
        return {'loss': avg_loss, **metrics}
    
    def train(self) -> Dict[str, Any]:
        """æ‰§è¡Œå®Œæ•´çš„è®­ç»ƒè¿‡ç¨‹"""
        logger.info("å¼€å§‹è®­ç»ƒ...")
        logger.info(f"è®¾å¤‡: {self.device}")
        logger.info(f"è®­ç»ƒæ ·æœ¬æ•°: {len(self.train_loader.dataset)}")
        logger.info(f"éªŒè¯æ ·æœ¬æ•°: {len(self.val_loader.dataset)}")
        
        start_time = time.time()
        
        for epoch in range(self.config.training.num_epochs):
            self.current_epoch = epoch
            epoch_start_time = time.time()
            
            # è®­ç»ƒé˜¶æ®µ
            train_results = self.train_epoch()
            self.train_history['loss'].append(train_results['loss'])     #æ¯ä¸ªepochçš„è®­ç»ƒæŸå¤±
            self.train_history['metrics'].append({k: v for k, v in train_results.items() if k != 'loss'})  #æ¯ä¸ªepochçš„è®­ç»ƒæŒ‡æ ‡
            
            # éªŒè¯é˜¶æ®µ
            val_results = self.validate_epoch()
            self.val_history['loss'].append(val_results['loss'])
            self.val_history['metrics'].append({k: v for k, v in val_results.items() if k != 'loss'})
            
            # å­¦ä¹ ç‡è°ƒåº¦
            warmup_epochs = getattr(self.config.training, 'warmup_epochs', 0)
            
            if epoch < warmup_epochs and self.warmup_scheduler:
                # Warmupé˜¶æ®µ
                self.warmup_scheduler.step()
            elif self.scheduler:
                # æ­£å¸¸è°ƒåº¦
                if isinstance(self.scheduler, optim.lr_scheduler.ReduceLROnPlateau):
                    self.scheduler.step(val_results['loss'])
                else:
                    self.scheduler.step()
            
            # è®°å½•åˆ°TensorBoard
            if self.writer:
                self.writer.add_scalar('Train/Loss', train_results['loss'], epoch)
                self.writer.add_scalar('Val/Loss', val_results['loss'], epoch)
                for metric_name, metric_value in train_results.items():
                    if metric_name != 'loss':
                        self.writer.add_scalar(f'Train/{metric_name}', metric_value, epoch)
                for metric_name, metric_value in val_results.items():
                    if metric_name != 'loss':
                        self.writer.add_scalar(f'Val/{metric_name}', metric_value, epoch)
            
            # æ£€æŸ¥å¹¶ä¿å­˜æœ€ä½³æ£€æŸ¥ç‚¹
            is_best = val_results['loss'] < self.best_val_loss   
            if is_best:
                self.best_val_loss = val_results['loss']
            
            if epoch % self.config.logging.save_every == 0 or is_best:
                self.checkpoint_manager.save_checkpoint(
                    epoch=epoch,
                    model=self.model,
                    optimizer=self.optimizer,
                    scheduler=self.scheduler,
                    loss=val_results['loss'],
                    metrics=val_results,
                    is_best=is_best
                )
            
            # æ—©åœæ£€æŸ¥
            if self.early_stopping:
                # è·å–ç›‘æ§æŒ‡æ ‡çš„å€¼
                monitor_value = self._get_monitor_value(val_results)
                
                # è°ƒç”¨æ—©åœï¼ˆä¼ å…¥æ­£ç¡®çš„å‚æ•°ï¼‰
                self.early_stopping(
                    current_value=monitor_value,
                    model_weights=self.model.state_dict()
                )
                
                if self.early_stopping.should_stop():
                    logger.info(f"ğŸ›‘ æ—©åœè§¦å‘ï¼Œåœ¨ç¬¬ {epoch} è½®åœæ­¢è®­ç»ƒ")
                    logger.info(f"ğŸ“Š ç›‘æ§æŒ‡æ ‡: {self.monitor_metric}")
                    
                    # æ¢å¤æœ€ä½³æ¨¡å‹æƒé‡
                    if self.early_stopping.restore_best_weights:
                        if self.early_stopping.restore_best_model(self.model):
                            logger.info(f"âœ… å·²å°†æ¨¡å‹æ¢å¤åˆ°æœ€ä½³çŠ¶æ€")
                            # ä½¿ç”¨æœ€ä½³æƒé‡é‡æ–°è®¡ç®—éªŒè¯æŒ‡æ ‡
                            val_results = self.validate_epoch()
                            logger.info(f"ğŸ“ˆ æœ€ä½³æ¨¡å‹éªŒè¯ç»“æœ - Loss: {val_results['loss']:.6f}, MSE: {val_results.get('mse', 0):.6f}")
                        else:
                            logger.warning(f"âš ï¸  æ— æ³•æ¢å¤æœ€ä½³æ¨¡å‹æƒé‡")
                    
                    # æ‰“å°æ—©åœæ‘˜è¦
                    logger.info("\n" + self.early_stopping.summary())
                    break
            
            # æ‰“å°epochç»“æœ
            epoch_time = time.time() - epoch_start_time
            logger.info(f'Epoch {epoch}/{self.config.training.num_epochs} - '
                       f'Time: {epoch_time:.2f}s - '
                       f'Train Loss: {train_results["loss"]:.6f} - '
                       f'Val Loss: {val_results["loss"]:.6f} - '
                       f'Val MSE: {val_results.get("mse", 0):.6f}')
        
        total_time = time.time() - start_time
        logger.info(f"âœ… è®­ç»ƒå®Œæˆï¼Œæ€»ç”¨æ—¶: {total_time:.2f}ç§’")
        
        # å¦‚æœæ²¡æœ‰è§¦å‘æ—©åœï¼Œæ‰“å°æ—©åœæœºåˆ¶çš„æ‘˜è¦
        if self.early_stopping and not self.early_stopping.should_stop():
            logger.info("\n" + self.early_stopping.summary())
        
        # å…³é—­TensorBoardå†™å…¥å™¨
        if self.writer:
            self.writer.close()
        
        # è¿”å›è®­ç»ƒç»“æœ
        result = {
            'train_history': self.train_history,
            'val_history': self.val_history,
            'best_val_loss': self.best_val_loss,
            'total_time': total_time
        }
        
        # æ·»åŠ æ—©åœä¿¡æ¯
        if self.early_stopping:
            result['early_stopping_info'] = self.early_stopping.get_info()
        
        return result
    
    def evaluate(self) -> Dict[str, float]:
        """åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹ï¼ˆå†…å­˜ä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
        if self.test_loader is None:
            logger.warning("æ²¡æœ‰æä¾›æµ‹è¯•æ•°æ®åŠ è½½å™¨")
            return {}
        
        logger.info("å¼€å§‹æµ‹è¯•é›†è¯„ä¼°...")
        
        self.model.eval()
        total_loss = 0.0
        
        # å†…å­˜ä¼˜åŒ–ï¼šåœ¨çº¿è®¡ç®—metrics
        compute_metrics_online = getattr(self.config.training, 'compute_metrics_online', True)
        
        if compute_metrics_online:
            num_samples = 0
            sum_squared_error = 0.0
            sum_absolute_error = 0.0
        else:
            all_predictions = []
            all_targets = []
        
        with torch.no_grad():
            for batch_idx, (emg_data, angle_data) in enumerate(self.test_loader):
                # æ•°æ®ç§»åˆ°è®¾å¤‡
                emg_data = emg_data.to(self.device)
                angle_data = angle_data.to(self.device)
                
                # å‰å‘ä¼ æ’­
                predictions = self.model(emg_data)
                loss = self.criterion(predictions, angle_data)
                
                # ç»Ÿè®¡æŸå¤±
                total_loss += loss.item()
                
                # åœ¨çº¿è®¡ç®—metrics
                if compute_metrics_online:
                    batch_size = predictions.shape[0]
                    num_samples += batch_size * predictions.shape[1]
                    
                    squared_error = ((predictions - angle_data) ** 2).sum().item()
                    absolute_error = torch.abs(predictions - angle_data).sum().item()
                    
                    sum_squared_error += squared_error
                    sum_absolute_error += absolute_error
                else:
                    all_predictions.append(predictions.cpu().numpy())
                    all_targets.append(angle_data.cpu().numpy())
                
                # å†…å­˜æ¸…ç†
                del emg_data, angle_data, predictions, loss
                
                if batch_idx % 50 == 0:
                    torch.cuda.empty_cache() if torch.cuda.is_available() else None
        
        # è®¡ç®—å¹³å‡æŸå¤±å’ŒæŒ‡æ ‡
        avg_loss = total_loss / len(self.test_loader)
        
        if compute_metrics_online:
            metrics = {
                'mse': sum_squared_error / num_samples,
                'mae': sum_absolute_error / num_samples,
                'rmse': np.sqrt(sum_squared_error / num_samples)
            }
        else:
            # å†…å­˜ä¼˜åŒ–ï¼šé™åˆ¶æ ·æœ¬æ•°
            if len(all_predictions) > 500:
                all_predictions = all_predictions[:500]
                all_targets = all_targets[:500]
            
            all_predictions = np.concatenate(all_predictions, axis=0)
            all_targets = np.concatenate(all_targets, axis=0)
            metrics = calculate_metrics(all_targets, all_predictions)
        
        test_results = {'loss': avg_loss, **metrics}
        
        logger.info("æµ‹è¯•é›†è¯„ä¼°ç»“æœ:")
        for metric_name, metric_value in test_results.items():
            logger.info(f"  {metric_name}: {metric_value:.6f}")
        
        return test_results
    
    def _get_monitor_value(self, results: Dict[str, float]) -> float:
        """
        ä»ç»“æœå­—å…¸ä¸­è·å–ç›‘æ§æŒ‡æ ‡çš„å€¼
        
        Args:
            results: åŒ…å«å„ç§æŒ‡æ ‡çš„ç»“æœå­—å…¸
            
        Returns:
            ç›‘æ§æŒ‡æ ‡çš„å€¼
        """
        if self.monitor_metric is None:
            return results.get('loss', float('inf'))
        
        # å»é™¤val_å‰ç¼€ï¼ˆå¦‚æœæœ‰ï¼‰
        metric_key = self.monitor_metric.replace('val_', '')
        
        # å°è¯•è·å–æŒ‡æ ‡å€¼
        if metric_key in results:
            return results[metric_key]
        else:
            logger.warning(f"âš ï¸  ç›‘æ§æŒ‡æ ‡ '{self.monitor_metric}' æœªæ‰¾åˆ°ï¼Œä½¿ç”¨ 'loss' ä»£æ›¿")
            return results.get('loss', float('inf'))
