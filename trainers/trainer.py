"""
æ¨¡å‹è®­ç»ƒå™¨
"""

import os
import time
import logging
from typing import Dict, Any, Optional, Tuple
import numpy as np

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader

from config import Config
from utils.metrics import calculate_metrics
from utils.checkpoint import CheckpointManager
from utils.early_stopping import EarlyStopping

logger = logging.getLogger(__name__)

# TensorBoardå¯¼å…¥å¤„ç†
try:
    from torch.utils.tensorboard import SummaryWriter
    TENSORBOARD_AVAILABLE = True
except ImportError:
    SummaryWriter = None
    TENSORBOARD_AVAILABLE = False
    logger.warning("TensorBoardä¸å¯ç”¨ï¼Œå°†è·³è¿‡å¯è§†åŒ–æ—¥å¿—")


class EMG2PoseTrainer:
    """EMGåˆ°å§¿æ€é¢„æµ‹çš„è®­ç»ƒå™¨"""
    
    def __init__(
        self,
        model: nn.Module,
        config: Config,
        train_loader: DataLoader,
        val_loader: DataLoader,
        test_loader: Optional[DataLoader] = None
    ):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        Args:
            model: è¦è®­ç»ƒçš„æ¨¡å‹
            config: é…ç½®å¯¹è±¡
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨
            test_loader: æµ‹è¯•æ•°æ®åŠ è½½å™¨ï¼ˆå¯é€‰ï¼‰
        """
        self.model = model
        self.config = config
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.test_loader = test_loader
        # è®¾å¤‡é…ç½®
        self.device = torch.device(config.training.device)
        self.model.to(self.device)
        
        # æŸå¤±å‡½æ•°
        self.criterion = self._create_criterion()
        
        # ä¼˜åŒ–å™¨
        self.optimizer = self._create_optimizer()
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = self._create_scheduler()
        self.warmup_scheduler = self._create_warmup_scheduler() if config.training.warmup_epochs > 0 else None
        
        # æ—©åœæœºåˆ¶
        self.early_stopping = EarlyStopping(
            patience=config.training.patience,
            min_delta=config.training.min_delta,
            mode='min'
        ) if config.training.early_stopping else None
        
        # æ£€æŸ¥ç‚¹ç®¡ç†å™¨
        self.checkpoint_manager = CheckpointManager(
            checkpoint_dir=config.logging.checkpoint_dir,
            save_best_only=config.logging.save_best_only,
            monitor=config.logging.monitor,
            mode=config.logging.mode
        )
        
        # TensorBoardå†™å…¥å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if TENSORBOARD_AVAILABLE and SummaryWriter is not None:
            try:
                self.writer = SummaryWriter(config.logging.log_dir)
                logger.info(f"âœ… TensorBoardåˆå§‹åŒ–æˆåŠŸï¼Œæ—¥å¿—ç›®å½•: {config.logging.log_dir}")
            except Exception as e:
                logger.warning(f"âš ï¸ TensorBoardåˆå§‹åŒ–å¤±è´¥: {e}")
                self.writer = None
        else:
            self.writer = None
        
        # è®­ç»ƒçŠ¶æ€
        self.current_epoch = 0    # å½“å‰è®­ç»ƒçš„epoch
        self.global_step = 0     # å…¨å±€è®­ç»ƒæ­¥æ•°
        self.best_val_loss = float('inf')    # æœ€ä½³éªŒè¯æŸå¤±
        self.train_history = {'loss': [], 'metrics': []}    # è®­ç»ƒå†å²è®°å½•
        self.val_history = {'loss': [], 'metrics': []}   # éªŒè¯å†å²è®°å½•
        
    def _create_criterion(self):
        """åˆ›å»ºæŸå¤±å‡½æ•°"""
        loss_type = getattr(self.config.training, 'loss_type', 'mse')
        
        if loss_type.lower() == 'mse':
            return nn.MSELoss()
        elif loss_type.lower() == 'mae':
            return nn.L1Loss()
        elif loss_type.lower() == 'mse+mae':
            # ç»„åˆæŸå¤±
            mae_weight = getattr(self.config.training, 'mae_weight', 0.5)
            mse_criterion = nn.MSELoss()
            mae_criterion = nn.L1Loss()
            
            def combined_loss(pred, target):
                mse = mse_criterion(pred, target)
                mae = mae_criterion(pred, target)
                return mse + mae_weight * mae
            
            return combined_loss
        else:
            logger.warning(f"æœªçŸ¥çš„æŸå¤±ç±»å‹: {loss_type}ï¼Œä½¿ç”¨é»˜è®¤MSE")
            return nn.MSELoss()
    
    def _create_optimizer(self) -> optim.Optimizer:
        """åˆ›å»ºä¼˜åŒ–å™¨"""
        if self.config.training.optimizer.lower() == 'adam':
            return optim.Adam(
                self.model.parameters(),
                lr=self.config.training.learning_rate,
                weight_decay=self.config.training.weight_decay
            )
        elif self.config.training.optimizer.lower() == 'adamw':
            return optim.AdamW(
                self.model.parameters(),
                lr=self.config.training.learning_rate,
                weight_decay=self.config.training.weight_decay
            )
        elif self.config.training.optimizer.lower() == 'sgd':
            return optim.SGD(
                self.model.parameters(),     
                lr=self.config.training.learning_rate,
                momentum=0.9,
                weight_decay=self.config.training.weight_decay
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ä¼˜åŒ–å™¨: {self.config.training.optimizer}")
    
    def _create_scheduler(self) -> Optional[optim.lr_scheduler._LRScheduler]:
        """åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨"""
        scheduler_type = self.config.training.scheduler.lower()
        
        if scheduler_type == 'step':
            return optim.lr_scheduler.StepLR(
                self.optimizer,
                step_size=self.config.training.step_size,
                gamma=self.config.training.gamma
            )
        elif scheduler_type == 'cosine':
            return optim.lr_scheduler.CosineAnnealingLR(
                self.optimizer,
                T_max=self.config.training.num_epochs
            )
        elif scheduler_type == 'plateau':  
            return optim.lr_scheduler.ReduceLROnPlateau(
                self.optimizer,
                mode='min',
                factor=0.5,
                patience=5,
                verbose=True
            )
        elif scheduler_type == 'none':
            return None
        else:
            return None
    
    def _create_warmup_scheduler(self):
        """åˆ›å»ºwarmupå­¦ä¹ ç‡è°ƒåº¦å™¨"""
        warmup_epochs = getattr(self.config.training, 'warmup_epochs', 0)
        if warmup_epochs <= 0:
            return None
        
        # ä½¿ç”¨çº¿æ€§warmup
        def warmup_lambda(epoch):
            if epoch < warmup_epochs:
                return (epoch + 1) / warmup_epochs
            return 1.0
        
        return optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda=warmup_lambda)
    
    def train_epoch(self) -> Dict[str, float]:   
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()  # è®¾ç½®æ¨¡å‹ä¸ºè®­ç»ƒæ¨¡å¼
        total_loss = 0.0   # ç´¯è®¡æŸå¤±
        all_predictions = []   # é¢„æµ‹ç»“æœ
        all_targets = []  # çœŸå®æ ‡ç­¾
        
        # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
        accumulation_steps = getattr(self.config.training, 'gradient_accumulation_steps', 1)
        
        for batch_idx, (emg_data, angle_data) in enumerate(self.train_loader):
            # æ•°æ®ç§»åˆ°è®¾å¤‡
            emg_data = emg_data.to(self.device)
            angle_data = angle_data.to(self.device)
            
            # å‰å‘ä¼ æ’­
            predictions = self.model(emg_data)  # 2. å‰å‘ä¼ æ’­
            loss = self.criterion(predictions, angle_data) # 3. è®¡ç®—æŸå¤±
            
            # æ¢¯åº¦ç´¯ç§¯ï¼šæŸå¤±éœ€è¦é™¤ä»¥ç´¯ç§¯æ­¥æ•°
            loss = loss / accumulation_steps
            
            # åå‘ä¼ æ’­
            loss.backward()     # 4. åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦
            
            # æ¯accumulation_stepsæ­¥æˆ–æœ€åä¸€æ­¥æ‰æ›´æ–°å‚æ•°
            if (batch_idx + 1) % accumulation_steps == 0 or (batch_idx + 1) == len(self.train_loader):
                # æ¢¯åº¦è£å‰ª
                grad_clip_norm = getattr(self.config.training, 'gradient_clip_norm', 1.0)
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=grad_clip_norm)
                
                # ä¼˜åŒ–å™¨æ­¥éª¤
                self.optimizer.step()  # 5. ä¼˜åŒ–å™¨æ›´æ–°å‚æ•°
                self.optimizer.zero_grad()    # 6. æ¸…é›¶æ¢¯åº¦
            
            # ç»Ÿè®¡ï¼ˆä½¿ç”¨åŸå§‹lossï¼Œä¸æ˜¯é™¤ä»¥accumulation_stepsåçš„ï¼‰
            total_loss += loss.item() * accumulation_steps
            all_predictions.append(predictions.detach().cpu().numpy())
            all_targets.append(angle_data.detach().cpu().numpy())
            
            # å†…å­˜æ¸…ç†ï¼šåŠæ—¶é‡Šæ”¾ä¸éœ€è¦çš„tensor
            del emg_data, angle_data, predictions
            if batch_idx % 100 == 0:
                torch.cuda.empty_cache() if torch.cuda.is_available() else None
            
            # æ—¥å¿—è®°å½•   ä½¿ç”¨tensorboardè®°å½•è®­ç»ƒè¿‡ç¨‹
            if batch_idx % self.config.logging.log_every == 0:
                if self.writer:
                    self.writer.add_scalar('Train/BatchLoss', loss.item(), self.global_step)
                    self.writer.add_scalar('Train/LearningRate', 
                                         self.optimizer.param_groups[0]['lr'], self.global_step)
                
                logger.info(f'Epoch: {self.current_epoch}, Batch: {batch_idx}/{len(self.train_loader)}, '
                           f'Loss: {loss.item():.6f}')
            
            self.global_step += 1
        
        # è®¡ç®—epochå¹³å‡æŸå¤±å’ŒæŒ‡æ ‡
        avg_loss = total_loss / len(self.train_loader)      #å¹³å‡æŸå¤±
        
        # ğŸ”´ å†…å­˜ä¼˜åŒ–ï¼šåˆ†æ‰¹è®¡ç®—metricsï¼Œé¿å…å¤§æ•°ç»„æ‹¼æ¥
        # åªåœ¨å‰1000ä¸ªbatchä¸Šè®¡ç®—metricsï¼ˆä»£è¡¨æ€§è¶³å¤Ÿï¼‰
        if len(all_predictions) > 1000:
            all_predictions = all_predictions[:1000]
            all_targets = all_targets[:1000]
        
        all_predictions = np.concatenate(all_predictions, axis=0) #å°†æ‰¹æ¬¡çš„é¢„æµ‹ç»“æœæ‹¼æ¥èµ·æ¥
        all_targets = np.concatenate(all_targets, axis=0) #å°†æ‰¹æ¬¡çš„çœŸå®æ ‡ç­¾æ‹¼æ¥èµ·æ¥
        metrics = calculate_metrics(all_targets, all_predictions) #è®¡ç®—æŒ‡æ ‡
        
        return {'loss': avg_loss, **metrics}
    
    def validate_epoch(self) -> Dict[str, float]:
        """éªŒè¯ä¸€ä¸ªepoch"""
        self.model.eval() # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
        total_loss = 0.0 # ç´¯è®¡æŸå¤±
        all_predictions = []
        all_targets = []
        
        with torch.no_grad(): # ç¦ç”¨æ¢¯åº¦è®¡ç®—
            for batch_idx, (emg_data, angle_data) in enumerate(self.val_loader):
                # æ•°æ®ç§»åˆ°è®¾å¤‡
                emg_data = emg_data.to(self.device)
                angle_data = angle_data.to(self.device)
                
                # å‰å‘ä¼ æ’­
                predictions = self.model(emg_data)
                loss = self.criterion(predictions, angle_data)
                
                # ç»Ÿè®¡
                total_loss += loss.item()
                all_predictions.append(predictions.cpu().numpy())
                all_targets.append(angle_data.cpu().numpy())
                
                # å†…å­˜æ¸…ç†
                del emg_data, angle_data, predictions
                if batch_idx % 50 == 0:
                    torch.cuda.empty_cache() if torch.cuda.is_available() else None
        
        # è®¡ç®—epochå¹³å‡æŸå¤±å’ŒæŒ‡æ ‡
        avg_loss = total_loss / len(self.val_loader)
        
        # ğŸ”´ å†…å­˜ä¼˜åŒ–ï¼šé™åˆ¶ç”¨äºmetricsè®¡ç®—çš„æ ·æœ¬æ•°
        # éªŒè¯é›†é€šå¸¸è¾ƒå°ï¼Œä½†ä»éœ€è¦é™åˆ¶ä»¥é¿å…OOM
        if len(all_predictions) > 500:
            all_predictions = all_predictions[:500]
            all_targets = all_targets[:500]
        
        all_predictions = np.concatenate(all_predictions, axis=0)  #å°†æ‰¹æ¬¡çš„é¢„æµ‹ç»“æœæ‹¼æ¥èµ·æ¥
        all_targets = np.concatenate(all_targets, axis=0)  #å°†æ‰¹æ¬¡çš„çœŸå®æ ‡ç­¾æ‹¼æ¥èµ·æ¥
        metrics = calculate_metrics(all_targets, all_predictions)  #è®¡ç®—æŒ‡æ ‡å¹¶ä¿å­˜åœ¨ä¸€ä¸ªå­—å…¸ä¸­ï¼ˆæŒ‡æ ‡åœ¨metricsä¸­ï¼‰
        
        return {'loss': avg_loss, **metrics}    #è¿”å›åŒ…å«æŸå¤±å’ŒæŒ‡æ ‡çš„å­—å…¸
    
    def train(self) -> Dict[str, Any]:
        """æ‰§è¡Œå®Œæ•´çš„è®­ç»ƒè¿‡ç¨‹"""
        logger.info("å¼€å§‹è®­ç»ƒ...")
        logger.info(f"è®¾å¤‡: {self.device}")
        logger.info(f"è®­ç»ƒæ ·æœ¬æ•°: {len(self.train_loader.dataset)}")
        logger.info(f"éªŒè¯æ ·æœ¬æ•°: {len(self.val_loader.dataset)}")
        
        start_time = time.time()
        
        for epoch in range(self.config.training.num_epochs):
            self.current_epoch = epoch
            epoch_start_time = time.time()
            
            # è®­ç»ƒé˜¶æ®µ
            train_results = self.train_epoch()
            self.train_history['loss'].append(train_results['loss'])     #æ¯ä¸ªepochçš„è®­ç»ƒæŸå¤±
            self.train_history['metrics'].append({k: v for k, v in train_results.items() if k != 'loss'})  #æ¯ä¸ªepochçš„è®­ç»ƒæŒ‡æ ‡
            
            # éªŒè¯é˜¶æ®µ
            val_results = self.validate_epoch()
            self.val_history['loss'].append(val_results['loss'])
            self.val_history['metrics'].append({k: v for k, v in val_results.items() if k != 'loss'})
            
            # å­¦ä¹ ç‡è°ƒåº¦
            warmup_epochs = getattr(self.config.training, 'warmup_epochs', 0)
            
            if epoch < warmup_epochs and self.warmup_scheduler:
                # Warmupé˜¶æ®µ
                self.warmup_scheduler.step()
            elif self.scheduler:
                # æ­£å¸¸è°ƒåº¦
                if isinstance(self.scheduler, optim.lr_scheduler.ReduceLROnPlateau):
                    self.scheduler.step(val_results['loss'])
                else:
                    self.scheduler.step()
            
            # è®°å½•åˆ°TensorBoard
            if self.writer:
                self.writer.add_scalar('Train/Loss', train_results['loss'], epoch)
                self.writer.add_scalar('Val/Loss', val_results['loss'], epoch)
                for metric_name, metric_value in train_results.items():
                    if metric_name != 'loss':
                        self.writer.add_scalar(f'Train/{metric_name}', metric_value, epoch)
                for metric_name, metric_value in val_results.items():
                    if metric_name != 'loss':
                        self.writer.add_scalar(f'Val/{metric_name}', metric_value, epoch)
            
            # æ£€æŸ¥å¹¶ä¿å­˜æœ€ä½³æ£€æŸ¥ç‚¹
            is_best = val_results['loss'] < self.best_val_loss   
            if is_best:
                self.best_val_loss = val_results['loss']
            
            if epoch % self.config.logging.save_every == 0 or is_best:
                self.checkpoint_manager.save_checkpoint(
                    epoch=epoch,
                    model=self.model,
                    optimizer=self.optimizer,
                    scheduler=self.scheduler,
                    loss=val_results['loss'],
                    metrics=val_results,
                    is_best=is_best
                )
            
            # æ—©åœæ£€æŸ¥
            if self.early_stopping:
                self.early_stopping(val_results['loss'])
                if self.early_stopping.should_stop():
                    logger.info(f"æ—©åœè§¦å‘ï¼Œåœ¨ç¬¬ {epoch} è½®åœæ­¢è®­ç»ƒ")
                    break
            
            # æ‰“å°epochç»“æœ
            epoch_time = time.time() - epoch_start_time
            logger.info(f'Epoch {epoch}/{self.config.training.num_epochs} - '
                       f'Time: {epoch_time:.2f}s - '
                       f'Train Loss: {train_results["loss"]:.6f} - '
                       f'Val Loss: {val_results["loss"]:.6f} - '
                       f'Val MSE: {val_results.get("mse", 0):.6f}')
        
        total_time = time.time() - start_time
        logger.info(f"è®­ç»ƒå®Œæˆï¼Œæ€»ç”¨æ—¶: {total_time:.2f}ç§’")
        
        # å…³é—­TensorBoardå†™å…¥å™¨
        if self.writer:
            self.writer.close()
        
        return {
            'train_history': self.train_history,
            'val_history': self.val_history,
            'best_val_loss': self.best_val_loss,
            'total_time': total_time
        }
    
    def evaluate(self) -> Dict[str, float]:
        """åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹"""
        if self.test_loader is None:
            logger.warning("æ²¡æœ‰æä¾›æµ‹è¯•æ•°æ®åŠ è½½å™¨")
            return {}
        
        logger.info("å¼€å§‹æµ‹è¯•é›†è¯„ä¼°...")
        
        self.model.eval()
        total_loss = 0.0
        all_predictions = []
        all_targets = []
        
        with torch.no_grad():
            for emg_data, angle_data in self.test_loader:
                # æ•°æ®ç§»åˆ°è®¾å¤‡
                emg_data = emg_data.to(self.device)
                angle_data = angle_data.to(self.device)
                
                # å‰å‘ä¼ æ’­
                predictions = self.model(emg_data)
                loss = self.criterion(predictions, angle_data)
                
                # ç»Ÿè®¡
                total_loss += loss.item()
                all_predictions.append(predictions.cpu().numpy())
                all_targets.append(angle_data.cpu().numpy())
        
        # è®¡ç®—å¹³å‡æŸå¤±å’ŒæŒ‡æ ‡
        avg_loss = total_loss / len(self.test_loader)
        
        # ğŸ”´ å†…å­˜ä¼˜åŒ–ï¼šé™åˆ¶ç”¨äºmetricsè®¡ç®—çš„æ ·æœ¬æ•°
        if len(all_predictions) > 500:
            all_predictions = all_predictions[:500]
            all_targets = all_targets[:500]
        
        all_predictions = np.concatenate(all_predictions, axis=0)
        all_targets = np.concatenate(all_targets, axis=0)
        metrics = calculate_metrics(all_targets, all_predictions)
        
        test_results = {'loss': avg_loss, **metrics}
        
        logger.info("æµ‹è¯•é›†è¯„ä¼°ç»“æœ:")
        for metric_name, metric_value in test_results.items():
            logger.info(f"  {metric_name}: {metric_value:.6f}")
        
        return test_results
